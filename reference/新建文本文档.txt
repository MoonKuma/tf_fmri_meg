#### 段落5：Image understanding with deep convolutional networks（深度学习网络在图像理解中的应用）

?	这一段并没有什么实际的内容，而是主要以炫耀作者自己的CNN被多少大型公司所采用为主。却是如作者所言，因为CNN在图片加工上带来的突破性进展，带来很很多实际的变革，也改变了很多大公司今后的计划。例如google利用CNN实现了自己的初代AI，alpha-go，只是这里有两点不同，第一alpha-go是强化学习，而不是作者所指的监督学习，其二google的CNN模型里面是只有卷积，没有池化的，原因是围棋盘上每一个子的信息都需要得到保存，而池化降维会导致重要信息的丢失。此外本文的作者Hilton后来也凭借自己在相关领域的杰出贡献，接任了google的人工智能研究组的组长工作。而另外一个代表是IBM公司，因为在各项业务上都没办法拔得头筹，所以IBM几乎变卖了自己的绝大部分业务，紧缩人员开始集中搞起了AI研究，并期待从中有所突破。至于显卡大厂英伟达（NVIDIA），近几年则为了抢占AI的硬件市场开发研究了大量专门用来机器学习的显卡（特斯拉系列，大概需要2W-7W一张普通的特斯拉卡），这种显卡具有强大的点阵计算能力，不过已经不再具有视频输出口，丧失了显卡最基本的现象功能了。

?	Deep-learning之所以会在这几年突然火爆起来，Hilton的CNN的贡献功不可没。不过很大原因也是因为随着智能设备（手机，电脑）和网络的普及，数据越来越多，获取数据也越来越方便。另一方面硬件设备的飞速发展也让之前理论上需要计算几年的模型可以在几分钟之内得到结果。两大优势，再加上CNN相对傻瓜（不需要对数据进行过多的预处理，也不需要应用者由强大的数学知识）的特点，使得CNN如雨后春笋在各行各业蓬勃发展起来。

?	在段落2中，他提到了两个用来防止过拟合的方法，一个是dropout，指的是训练的时候，每次随机关闭一部分神经元（如果部分神经元就可以表现的一样好，那就不需要全部的神经元了）。另一个是data augumentation，指的是在使用原始图片训练的同时，也是用原始图片剪裁后的图片训练（如果对少量信息也可以得到一样好的预测结果，就需要解读全部信息了）。这两种方法现在已经基本成了CNN的标配了。

#### 段落6：Distributed representations and language processing（分布表征与语言处理）

*注：后面的内容是Deep-learning的另一个分支，RNN分支，RNN是用来处理连续体数据的，常见的如语音，语言等，我们的图像数据分析里面基本上不会用到这部分内容。不过这个也许会作为之后其他的分析的一些启蒙，特别其中的LSTM（long-short term memory）的概念，也是一定程度上借用了记忆相关的概念。

?	语言具有分布表征的特点，如果理解这种特点，以及如何表征语言，也就是各家机器学习争论的核心。

?	关于分布表征，作者在文中是这样解释的。

```python
'''
When trained to predict the next word in a news story, for example, the learned word vectors for Tuesday and Wednesday are very similar, as are the word vectors for Sweden and Norway. Such representations are called distributed representations because their elements (the features) are not mutually exclusive and their many configurations correspond to the variations seen in the observed data. These word vectors are composed of learned features that were not determined ahead of time by experts, but automatically discovered by the neural network. Vector representations of words learned from text are now very widely used in natural language applications.
'''
```

?	考虑一个切实的情况也许更容易理解一些，当出现一个英文句子填词，I want to go shopping on ___, 的时候，填入Wednesday（或者Tuesday/Holloween/Christmas）是合适的，填入apple/cat显然是不合适的。这说明Tuesday/Holloween/Christmas/Wednesday在某种意义上（虽然他们表面上看起来大相径庭）具有相似的表征（representation，这里表征的是一个确定的日期），这就是所谓的分布表征，不同的词，可以通过一定的表征方法，建立起他们之间的关联。

?	分布表征是所有语言学习共同的概念，deep-learning和传统方法的争论主要在于如何表征的问题上。

?	这里作者提到了一种叫做N-grams的方法，其实这也是之前最常用的自然语言处理的方法，这里简单介绍一下。假如有一个问题，判断以下句子的合法性，I wish to eat computer keyboard with Mary. 要如何判断呢，在2-grams的方法下，是这样进行处理的。句子的合理性 = 句子中所有词同时出现的合理性= 第一个词出现的概率x第一个词的基础上第二个词出现的概率x前两个词的基础上第三个词出现的概率....

?        写成公式是这样的

```python
s = 'I wish to eat computer keyboard with Mary'
P(s) = P(w1,w2,w3....w8) = p(w1) * p(w2|w1) * p(w3|w1,w2)...*p(w8|w1*w2*...*w7)
```

?       但是这个公式一旦展开计算就会变得十分复杂，所以2-grams相当于对于他做了一次简化，即认为只有相邻两个词（如果是3-gram就是相邻3个了，但是随着判断相邻的数量增加，计算量和训练量都会变得庞大起来）会互相影响，结果就把公式简化成了

```python
P(s) = P(w1,w2,w3....w8) = p(w1) * p(w2|w1) * p(w3|w2)...*p(w8|w7)
```

?       至于每个概率是多少，就需要训练的过程学习了，一般训练是通过通篇解析各种小说文本等，学习各种组合，校准每种组合出现的概率，最后发现这个句子虽然机会每个配对都表现不错，但是其中的一对 p(computer|eat)概率太低了，因为是乘法关系，所以显著拉低了整个句子的准确性，这样最终给出了不合法的评判。

?	2-grams是最常用的组合，当然也可以扩展到3-grams，4-grams或者更多，不过，就如文中提到的，对于一个总词数是V的词表，N-grams对应得组合数量是V^N，如果是泛用性的词库，例如搜狗的联想搜索，其中可能涉及到10000+的词，那10000^2 到10000^3就不是所有用于能承受的了。况且为了训练出绝大部分这种三连词的组合，也需要更多地训练集。而且，N-grams局限于N也无法解决距离比较远的线索问题。

?	需要说明，虽然在文中作者对于N-grams抱有一定程度上的贬低，不过（在落后的年代），N-grams是一个被认为普遍有效的方法，解决过很多如word的自动改错，自动联想等等。对于N-grams来说，表征可以理解成词对出现的概率，或者词之间的距离（apple和orange距离近，和April距离远）。而对于神经网络来说，这个表征则是高级神经元对于语义的抽象（尽管作者也说不清楚抽象的东西是什么），但是他被证实在处理一些复杂语言问题（例如翻译等）上有着更明显的优势。